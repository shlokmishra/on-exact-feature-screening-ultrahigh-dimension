# Print results
print(paste("Max y:", max_y))
print(paste("Min x:", min_x, "Max x:", max_x))
# Function to perform KS test for all class pairs in a multi-class setting
multiClassKS <- function(train.set, class.labels) {
num.classes <- length(unique(class.labels))
d <- ncol(train.set)
# Initialize a matrix to store max KS statistics for each feature across class pairs
maxKSStats <- rep(0, d)
for (l in 1:(num.classes - 1)) {
for (m in (l + 1):num.classes) {
# Indices for classes l and m
idxL <- which(class.labels == l)
idxM <- which(class.labels == m)
# Combine the subsets for the two classes
combinedSet <- rbind(train.set[idxL, ], train.set[idxM, ])
# Compute KS statistics for the combined set
ksStats <- ks_test_1d(combinedSet, length(idxL), length(idxM))
# Update max KS statistics if current stats are higher
maxKSStats <- pmax(maxKSStats, ksStats)
}
}
# Logic to determine significant features can be adapted as needed
# For illustration, simply returning the features with KS stats above a threshold
threshold <- 0.05 # Example threshold for significant KS statistic
sigFeatures <- which(maxKSStats > threshold)
return(sigFeatures)
}
# Assuming train.set is your dataset and class.labels is a vector of class labels
# You would call the function like this:
sigFeatures <- multiClassKS(train.set, class.labels)
n1train
#------------------------------------------------------------------------------
set.seed(123)
REP = 100 # number of iterations to run.
trn.sz.perClass = 100 # size of training sample from each class in all simulated examples Ex 1- 16 except Ex 8
tst.sz.perClass = 250 # size of test sample from each class in all simulated examples
# OS.var = readline(prompt = 'If you are on Windows, please type "W". Otherwise type "O": ')
# cluster.type = ifelse(OS.var == 'W', 'PSOCK', 'FORK')
cluster.type = 'FORK'
#------------------------------------------------------------------------------
EXNAMES = paste('ex', c(5,6), sep = '')
#------------------------------------------------------------------------------
PCKG = c(
# 'energy',
'proxy',
'parallelDist',
'nbpMatching',
'MASS',
'doParallel',
'plot.matrix',
'RcppArmadillo',
'RcppXPtrUtils',
'inline',
# 'VariableScreening',
'utils',
'e1071',
'Peacock.test',
'ks',
'nprobust'
)
# install.packages(setdiff(PCKG, rownames(installed.packages())))
lapply(PCKG, library, character.only = T)
#------------------------------------------------------------------------------
setwd("~/code/on-exact-feature-screening-ultrahigh-dimension/scripts")
source('../relevant_functions/relevant_functions.R')
source('../relevant_functions/modifications.R')
cl = makeCluster(no.cores, type = cluster.type)
registerDoParallel(cl)
clusterExport(cl, as.character(lsf.str())) # exporting all user defined functions
i = 1
flg = 1
RESULT = list(0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0)
names(RESULT) = EXNAMES
out = NULL
pb = ERR = NULL
gamfuns = c('gexp', 'gsqrt', 'glog') #choice of gamma functions
d = 10 #dimension
revmu = 0.75 # the difference in mean for the marginal signal in examples 9-16
#------------------------------------------------------------------------------
EXNAMES = paste('ex', c(1,2), sep = '')
exID = EXNAMES[1]
switch (
exID,
ex1 = { # Extended example with three classes for a location problem with marginal features.
block.size = 2
s = 2
no.s = 4 # 4 marginals
sig.pos = c(1, 3) # seq(1, length.out = s, by = block.size)
n1train = n2train = n3train = trn.sz.perClass
ntrain = n1train + n2train + n3train
n1test = n2test = n3test = tst.sz.perClass
n1 = n1train + n1test
n2 = n2train + n2test
n3 = n3train + n3test
n = n1 + n2 + n3 # Adjust total sample size to account for the third class
prms = t(rperm(m = 50 * ntrain, size = ntrain))
},
ex2 = { # Extended example 2 from the manuscript to include a third class. A scale problem with paired features.
block.size = 2
s = 2
no.s = 4 # 2 pairs
sig.pos = c(1, 3)
n1train = n2train = n3train = trn.sz.perClass
ntrain = n1train + n2train + n3train
n1test = n2test = n3test = tst.sz.perClass
n1 = n1train + n1test
n2 = n2train + n2test
n3 = n3train + n3test
n = n1 + n2 + n3 # Adjusted for three classes
prms = t(rperm(m = 50 * ntrain, size = ntrain))
}
)
clusterExport(
cl,
c(
'revmu',
'n1train',
'n2train',
'n',
'ntrain',
'n1test',
'n2test',
'n1',
'n2',
'd',
'block.size'
)
)
switch (
exID,
ex1 = {
pop1 = matrix(rnorm(n1 * d, mean = 0, sd = sqrt(1)), ncol = d)
pop2 = cbind(matrix(rnorm(n2 * 4, mean = 5, sd = sqrt(1)), ncol = 4), matrix(rnorm(n2 * (d-4), mean = 0, sd = sqrt(1)), ncol = d-4))
pop3 = cbind(matrix(rnorm(n3 * 4, mean = -5, sd = sqrt(1)), ncol = 4), matrix(rnorm(n3 * (d-4), mean = 0, sd = sqrt(1)), ncol = d-4))
},
ex2 = {
# Population 1
pop1 = matrix(rnorm(n1 * d, mean = 0, sd = 1), ncol = d)
# Generate correlated features for pop1
for (k in 1:n1) {
for (len in 1:length(sig.pos)) {
pop1[k, c(sig.pos[len] + (0:(block.size - 1)))] = mvrnorm(
n = 1,
mu = rep(0, block.size),
Sigma = constcor.mat(d = block.size, rho = 0.9)
)
}
}
# Population 2
pop2 = matrix(rnorm(n2 * d, mean = 0, sd = 1), ncol = d)
# Generate correlated features for pop2
for (k in 1:n2) {
for (len in 1:length(sig.pos)) {
pop2[k, c(sig.pos[len] + (0:(block.size - 1)))] = mvrnorm(
n = 1,
mu = rep(0, block.size),
Sigma = constcor.mat(d = block.size, rho = -0.9)
)
}
}
# Population 3
pop3 = matrix(rnorm(n3 * d, mean = 0, sd = 1), ncol = d)
# Generate correlated features for pop3
for (k in 1:n3) {
for (len in 1:length(sig.pos)) {
pop3[k, c(sig.pos[len] + (0:(block.size - 1)))] = mvrnorm(
n = 1,
mu = rep(0, block.size),
Sigma = constcor.mat(d = block.size, rho = 0.5)
)
}
}
}
)
########################### DATA GENERATION IS COMPLETE ##################
train1 = pop1[1:n1train,]
train2 = pop2[1:n2train,]
train3 = pop3[1:n3train,]  # New line for third class
# Combine training data from three classes
train.set  = rbind(train1, train2, train3)
# Combine testing data from three classes
test.set = rbind(pop1[n1train + (1:n1test),], pop2[n2train + (1:n2test),], pop3[n3train + (1:n3test),])
rm(pop1)
rm(pop2)
rm(pop3)  # Remove pop3 after creating test set
# Create labels for training and testing data
train.lab = rep(1:3, c(n1train, n2train, n3train))
test.lab = rep(1:3, c(n1test, n2test, n3test))
tmp = NULL
clusterExport(cl, c('train.set', 'test.set'))
registerDoParallel(cl)
no.cores = round(detectCores() - 1)
cl = makeCluster(no.cores, type = cluster.type)
registerDoParallel(cl)
clusterExport(cl, as.character(lsf.str())) # exporting all user defined functions
i = 1
flg = 1
RESULT = list(0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0)
names(RESULT) = EXNAMES
out = NULL
pb = ERR = NULL
gamfuns = c('gexp', 'gsqrt', 'glog') #choice of gamma functions
d = 10 #dimension
revmu = 0.75 # the difference in mean for the marginal signal in examples 9-16
switch (
exID,
ex1 = { # Extended example with three classes for a location problem with marginal features.
block.size = 2
s = 2
no.s = 4 # 4 marginals
sig.pos = c(1, 3) # seq(1, length.out = s, by = block.size)
n1train = n2train = n3train = trn.sz.perClass
ntrain = n1train + n2train + n3train
n1test = n2test = n3test = tst.sz.perClass
n1 = n1train + n1test
n2 = n2train + n2test
n3 = n3train + n3test
n = n1 + n2 + n3 # Adjust total sample size to account for the third class
prms = t(rperm(m = 50 * ntrain, size = ntrain))
},
ex2 = { # Extended example 2 from the manuscript to include a third class. A scale problem with paired features.
block.size = 2
s = 2
no.s = 4 # 2 pairs
sig.pos = c(1, 3)
n1train = n2train = n3train = trn.sz.perClass
ntrain = n1train + n2train + n3train
n1test = n2test = n3test = tst.sz.perClass
n1 = n1train + n1test
n2 = n2train + n2test
n3 = n3train + n3test
n = n1 + n2 + n3 # Adjusted for three classes
prms = t(rperm(m = 50 * ntrain, size = ntrain))
}
)
clusterExport(
cl,
c(
'revmu',
'n1train',
'n2train',
'n',
'ntrain',
'n1test',
'n2test',
'n1',
'n2',
'd',
'block.size'
)
)
ptm = proc.time()
pb <-
txtProgressBar(
initial = 1,
min = 1,
max = REP ,
style = 3,
width = REP,
char = '='
)
out2 <- data.frame(listS = character(REP), stringsAsFactors = FALSE)
switch (
exID,
ex1 = {
pop1 = matrix(rnorm(n1 * d, mean = 0, sd = sqrt(1)), ncol = d)
pop2 = cbind(matrix(rnorm(n2 * 4, mean = 5, sd = sqrt(1)), ncol = 4), matrix(rnorm(n2 * (d-4), mean = 0, sd = sqrt(1)), ncol = d-4))
pop3 = cbind(matrix(rnorm(n3 * 4, mean = -5, sd = sqrt(1)), ncol = 4), matrix(rnorm(n3 * (d-4), mean = 0, sd = sqrt(1)), ncol = d-4))
},
ex2 = {
# Population 1
pop1 = matrix(rnorm(n1 * d, mean = 0, sd = 1), ncol = d)
# Generate correlated features for pop1
for (k in 1:n1) {
for (len in 1:length(sig.pos)) {
pop1[k, c(sig.pos[len] + (0:(block.size - 1)))] = mvrnorm(
n = 1,
mu = rep(0, block.size),
Sigma = constcor.mat(d = block.size, rho = 0.9)
)
}
}
# Population 2
pop2 = matrix(rnorm(n2 * d, mean = 0, sd = 1), ncol = d)
# Generate correlated features for pop2
for (k in 1:n2) {
for (len in 1:length(sig.pos)) {
pop2[k, c(sig.pos[len] + (0:(block.size - 1)))] = mvrnorm(
n = 1,
mu = rep(0, block.size),
Sigma = constcor.mat(d = block.size, rho = -0.9)
)
}
}
# Population 3
pop3 = matrix(rnorm(n3 * d, mean = 0, sd = 1), ncol = d)
# Generate correlated features for pop3
for (k in 1:n3) {
for (len in 1:length(sig.pos)) {
pop3[k, c(sig.pos[len] + (0:(block.size - 1)))] = mvrnorm(
n = 1,
mu = rep(0, block.size),
Sigma = constcor.mat(d = block.size, rho = 0.5)
)
}
}
}
)
########################### DATA GENERATION IS COMPLETE ##################
train1 = pop1[1:n1train,]
train2 = pop2[1:n2train,]
train3 = pop3[1:n3train,]  # New line for third class
# Combine training data from three classes
train.set  = rbind(train1, train2, train3)
# Combine testing data from three classes
test.set = rbind(pop1[n1train + (1:n1test),], pop2[n2train + (1:n2test),], pop3[n3train + (1:n3test),])
rm(pop1)
rm(pop2)
rm(pop3)  # Remove pop3 after creating test set
# Create labels for training and testing data
train.lab = rep(1:3, c(n1train, n2train, n3train))
test.lab = rep(1:3, c(n1test, n2test, n3test))
tmp = NULL
clusterExport(cl, c('train.set', 'test.set'))
# Function to perform KS test for all class pairs in a multi-class setting
multiClassKS <- function(train.set, class.labels) {
num.classes <- length(unique(class.labels))
d <- ncol(train.set)
# Initialize a matrix to store max KS statistics for each feature across class pairs
maxKSStats <- rep(0, d)
for (l in 1:(num.classes - 1)) {
for (m in (l + 1):num.classes) {
# Indices for classes l and m
idxL <- which(class.labels == l)
idxM <- which(class.labels == m)
# Combine the subsets for the two classes
combinedSet <- rbind(train.set[idxL, ], train.set[idxM, ])
# Compute KS statistics for the combined set
ksStats <- ks_test_1d(combinedSet, length(idxL), length(idxM))
# Update max KS statistics if current stats are higher
maxKSStats <- pmax(maxKSStats, ksStats)
}
}
# Logic to determine significant features can be adapted as needed
# For illustration, simply returning the features with KS stats above a threshold
threshold <- 0.05 # Example threshold for significant KS statistic
sigFeatures <- which(maxKSStats > threshold)
return(sigFeatures)
}
# Assuming train.set is your dataset and class.labels is a vector of class labels
# You would call the function like this:
sigFeatures <- multiClassKS(train.set, train.lab)
sigFeatures
# Function to perform KS test for all class pairs in a multi-class setting
multiClassKS <- function(train.set, class.labels) {
num.classes <- length(unique(class.labels))
d <- ncol(train.set)
# Initialize a matrix to store max KS statistics for each feature across class pairs
maxKSStats <- rep(0, d)
for (l in 1:(num.classes - 1)) {
for (m in (l + 1):num.classes) {
# Indices for classes l and m
idxL <- which(class.labels == l)
idxM <- which(class.labels == m)
# Combine the subsets for the two classes
combinedSet <- rbind(train.set[idxL, ], train.set[idxM, ])
# Compute KS statistics for the combined set
ksStats <- ks_test_1d(combinedSet, length(idxL), length(idxM))
# Update max KS statistics if current stats are higher
maxKSStats <- pmax(maxKSStats, ksStats)
}
}
# Apply the logic for identifying significant features
sort.maxKSStats <- sort(maxKSStats)
b.rat <- abs(sort.maxKSStats[-1] / sort.maxKSStats[-length(sort.maxKSStats)])
Ecut <- (floor(length(b.rat) / 2)) + which.max(b.rat[-(1:(floor(length(b.rat) / 2)))])
sig.pos.est <- which(maxKSStats %in% sort.maxKSStats[(length(sort.maxKSStats) - Ecut + 1):length(sort.maxKSStats)])
return(sig.pos.est)
}
# Assuming train.set is your dataset and class.labels is a vector of class labels
# You would call the function like this:
# sigFeatures <- multiClassKS(train.set, class.labels)
# Assuming train.set is your dataset and class.labels is a vector of class labels
# You would call the function like this:
sigFeatures <- multiClassKS(train.set, train.lab)
sigFeatures
class.labels <- train.lab
class.labels
num.classes <- length(unique(class.labels))
d <- ncol(train.set)
num.classes
d
# Initialize a matrix to store max KS statistics for each feature across class pairs
maxKSStats <- rep(0, d)
maxKSStats
l=1
m=2
# Indices for classes l and m
print("l is", l, " and m is", m)
# Indices for classes l and m
print("l is" l " and m is" m)
# Indices for classes l and m
print(paste(“l is”, l))
print(paste(“m is”, m))
# Indices for classes l and m
print(paste("l is", l))
print(paste("m is", m))
idxL <- which(class.labels == l)
idxM <- which(class.labels == m)
idxL
idxM
# Combine the subsets for the two classes
combinedSet <- rbind(train.set[idxL, ], train.set[idxM, ])
# Compute KS statistics for the combined set
ksStats <- ks_test_1d(combinedSet, length(idxL), length(idxM))
ksStats
# Update max KS statistics if current stats are higher
maxKSStats <- pmax(maxKSStats, ksStats)
# Update max KS statistics if current stats are higher
maxKSStats <- pmax(maxKSStats, ksStats)
maxKSStats
?pmax
for (l in 1:(num.classes - 1)) {
for (m in (l + 1):num.classes) {
# Indices for classes l and m
print(paste("l is", l))
print(paste("m is", m))
idxL <- which(class.labels == l)
idxM <- which(class.labels == m)
# Combine the subsets for the two classes
combinedSet <- rbind(train.set[idxL, ], train.set[idxM, ])
# Compute KS statistics for the combined set
ksStats <- ks_test_1d(combinedSet, length(idxL), length(idxM))
# Update max KS statistics if current stats are higher
maxKSStats <- pmax(maxKSStats, ksStats)
}
}
# Apply the logic for identifying significant features
sort.maxKSStats <- sort(maxKSStats)
b.rat <- abs(sort.maxKSStats[-1] / sort.maxKSStats[-length(sort.maxKSStats)])
Ecut <- (floor(length(b.rat) / 2)) + which.max(b.rat[-(1:(floor(length(b.rat) / 2)))])
sig.pos.est <- which(maxKSStats %in% sort.maxKSStats[(length(sort.maxKSStats) - Ecut + 1):length(sort.maxKSStats)])
sig.pos.est
maxKSStats
sort.maxKSStats
plot(sort.maxKSStats)
num.classes <- length(unique(class.labels))
d <- ncol(train.set)
# Initialize a matrix to store max KS statistics for each feature across class pairs
maxKSStats <- rep(0, d)
for (l in 1:(num.classes - 1)) {
for (m in (l + 1):num.classes) {
# print(paste("l is", l))
# print(paste("m is", m))
# Indices for classes l and m
idxL <- which(class.labels == l)
idxM <- which(class.labels == m)
# Combine the subsets for the two classes
combinedSet <- rbind(train.set[idxL, ], train.set[idxM, ])
# Compute KS statistics for the combined set
ksStats <- ks_test_1d(combinedSet, length(idxL), length(idxM))
# Update max KS statistics if current stats are higher
maxKSStats <- pmax(maxKSStats, ksStats)
}
}
maxKSStats
margE <- maxKSStats
sort.margE = sort(margE)
b.rat = abs(sort.margE[-1] / sort.margE[-length(sort.margE)])
Ecut = (floor(length(b.rat) / 2)) + which.max(b.rat[-(1:(floor(length(b.rat) /
2)))])
sig.pos.est = which(margE %in% sort.margE[1:(length(sort.margE)) > Ecut]) # selected components
marg.var = sig.pos.est
marg.var
# Function to perform KS test for all class pairs in a multi-class setting
multiClassKS <- function(train.set, class.labels) {
num.classes <- length(unique(class.labels))
d <- ncol(train.set)
# Initialize a matrix to store max KS statistics for each feature across class pairs
maxKSStats <- rep(0, d)
for (l in 1:(num.classes - 1)) {
for (m in (l + 1):num.classes) {
# print(paste("l is", l))
# print(paste("m is", m))
# Indices for classes l and m
idxL <- which(class.labels == l)
idxM <- which(class.labels == m)
# Combine the subsets for the two classes
combinedSet <- rbind(train.set[idxL, ], train.set[idxM, ])
# Compute KS statistics for the combined set
ksStats <- ks_test_1d(combinedSet, length(idxL), length(idxM))
# Update max KS statistics if current stats are higher
maxKSStats <- pmax(maxKSStats, ksStats)
}
}
margE <- maxKSStats
# Apply the logic for identifying significant features
sort.margE = sort(margE)
b.rat = abs(sort.margE[-1] / sort.margE[-length(sort.margE)])
Ecut = (floor(length(b.rat) / 2)) + which.max(b.rat[-(1:(floor(length(b.rat) /2)))])
sig.pos.est = which(margE %in% sort.margE[1:(length(sort.margE)) > Ecut]) # selected components
return(sig.pos.est)
}
# Assuming train.set is your dataset and class.labels is a vector of class labels
# You would call the function like this:
# sigFeatures <- multiClassKS(train.set, class.labels)
# Assuming train.set is your dataset and class.labels is a vector of class labels
# You would call the function like this:
sigFeatures <- multiClassKS(train.set, train.lab)
marg_string <- paste(marg.var, collapse = ", ")
sigFeatures
